############ Time Series Forecasting ######################
# Set Working Direction
setwd("C:/Users/ddfer/Desktop/MSBA Misc 2/Machine Learning 2/TP02")
#
rm(list=ls())
library(forecast)
library(glue)
library(e1071)
library(ggplot2)
library(tidyverse)


train <- read.csv("train.csv")
test <- read.csv("test.csv")
train$item <- as.factor(train$item)
train$store <- as.factor(train$store)
train$date <- as.Date(train$date, "%m/%d/%Y")

test$item <- as.factor(test$item)
test$store <- as.factor(test$store)
test$date <- as.Date(test$date, "%m/%d/%Y")

attach(train)

all_data <- train %>% bind_rows(test)

#Seasonality for stores 1-10, item 2
ggplot(all_data %>% filter(item == 2), aes(x= as.Date(date), y = sales)) + 
  geom_line() + facet_wrap(~store)

summary(train)
dim(train)    
#913000 rows 4 columns

#Adjust Stores and items here
#Store 5 item 1
train_sample_onestore <- data.frame(train[store == 5 & item == 1,]) #Train Sample
sample1store <- ts(train_sample_onestore$sales, frequency = 365, start=c(2013,1))
plot(sample1store, ylab = "Sales", xlab = "Years", main = "Store 5 Item 1")
summary(sample1store)

train_sample_allstores <- data.frame(train[store == 1:10 & item == 1,]) #Train Sample
sampleallstores <- ts(train_sample_allstores$sales, frequency = 365, start=c(2013,1))
plot(sampleallstores, ylab = "Sales", xlab = "Years", main = "All Stores Item 1")
summary(sampleallstores)

train_sample <- data.frame(train[store == 1:10 & item == 1,]) #Train Sample
sample_ts <- ts(train_sample$sales, frequency = 365, start=c(2013,1))

#test_sample <- data.frame(test[store == 1:10 & item == 1,]) #Test Sample
#ndiffs(sample_ts)

###############################################################################
set.seed(1)
#Plots
#Naive Method
#Useful for highly seasonal data 
naiveTS <- naive(sample_ts, h = 90)
autoplot(naiveTS, ylab = "Sales", xlab = "Years", main = "Naive TS")
summary(naiveTS)

#Interpretation:
# Error measures:
#                       ME     RMSE     MAE       MPE     MAPE      MASE       ACF1
# Training set 0.007654456 8.301192 6.41006 -8.172359 32.64487 0.7651542 -0.5378866

#RMSE is high, therefore, it is not great at forecasting, MAE is high as well
#Naive Method - Estimating technique in which the last period's actual are used as this period's forecast, 
#without adjusting them or attempting to establish causal factors. 
#It is used only for comparison with the forecasts generated by the better (sophisticated) techniques.

#TS for one year (2013-2014)
sample_ts_1yr <- ts(train_sample$sales, frequency = 365, start=c(2013,1), end = c(2014, 1))
naiveTS_1yr <- naive(sample_ts_1yr, h = 90)
autoplot(naiveTS_1yr, ylab = "Sales", xlab = "Years", main = "Naive TS")
summary(naiveTS_1yr)


#SNaive Method 
snaiveTS <- snaive(sample_ts, h = 90)
autoplot(snaiveTS, ylab = "Sales", xlab = "Years", main = "SNaive TS")
summary(snaiveTS)

#Interpretation:
# Error measures:
#                    ME     RMSE      MAE        MPE     MAPE MASE        ACF1
# Training set 1.249144 6.716769 5.210815 -0.9289991 32.52945    1 -0.02773318

#RMSE and MAE are still high

#TS for _____
sample_ts_month <- ts(train_sample$sales, frequency = 365, start=c(2013,1), end = c(2013, 150))

naiveTS_month <- snaive(sample_ts_month, h = 90)
autoplot(naiveTS_month, ylab = "Sales", xlab = "Years", main = "Naive TS")
summary(naiveTS_month)


#Seasonal Decomposition
sample_ts_month <- ts(train_sample$sales, frequency = 365, start=c(2013,1))
stl_ts <- stl(sample_ts_month, s.window="period")
autoplot(stl_ts, ylab = "Sales", xlab = "Years", main = "Seasonal Decomp TS")
summary(stl_ts)

#STL is a versatile and robust method for decomposing TS. Stands for Seasonal and Trend
#decomposition using Loess. STL can handle any type of seasonality, not just monthly and quarterly
#robust to outliers, (unusual observations won't affect trend but will affect remainder) 
#Seasonal decomposition can be used to split the TS model into different components
#Data
#Trend - combination of trend and cycle
#Remainder - anything in the TS besides trend and seasonal


###############################################################################
#Exponential Smoothing
#Forecast based on a description of trend and seasonality within the data
#Simple Exponential Smoothing Method 
sesTS <- ses(sample_ts, h = 90)
autoplot(sesTS, ylab = "Sales", xlab = "Years", main = "SES TS")
summary(sesTS)

#Interpretation:
# Error measures:
#                      ME     RMSE      MAE       MPE     MAPE      MASE        ACF1
# Training set 0.01945157 6.556213 5.110421 -8.945879 27.12256 0.6100193 -0.07533508

#RMSE and MAE still quite high 
#Best use of for forecasting data with no clear trend of seasonal pattern
#Our data is clearly seasonal, so just wanted to present this to show it can be used

###############################################################################
#Holt winters forecasting & residuals
holty <- holt(sample_ts, h =90)
autoplot(holty, ylab = "Sales", xlab = "Years", main = "Holt TS")
checkresiduals(holty)
summary(holty)

#Interpretation
# Error measures:
#                        ME     RMSE     MAE       MPE     MAPE      MASE        ACF1
# Training set -0.006091873 6.557286 5.11242 -9.070797 27.15833 0.6102579 -0.07548854

#Holt winters is one of the most popular ways to do a time series forecast.
#This method comprises three smoothing equations for forecasting
#to model 3 aspects of time series, level, trend/slope, and seasonality

# for first graph, residuals are not correlated and have a 
#mean of zero therefore the residuals are useful and the Holt method is a good forecasting model
# Same exists as above for the bottom right graph
# However, for bottom left plot there with a huge amount of the data failing the
#box test, there looks to be some autocorrelation in the data

###############################################################################
#ARIMA
#Most widely used approaches to forecasting, they aim to describe the autocorrelations
#in the data
#Forecasting with ARIMA
Smodel1<-Arima(sample_ts,order=c(0,1,0), 
               seasonal=list(order=c(1,1,0),period=4)) 
Smodel1 #AIC=12766.63   AICc=12766.64   BIC=12777.64

Smodel2<-Arima(sample_ts,order=c(0,1,2), 
               seasonal=list(order=c(1,1,0),period=4)) 
Smodel2 #AIC=11538.43   AICc=11538.45   BIC=11560.46

model_sarima <- auto.arima(sample_ts)
model_sarima


acfmod <- Acf(model_sarima$residuals, lag.max = 100, ci=0.99, main="White Noise")     #Looks like white noise
autoplot(acfmod)

pacfmod <- Pacf(model_sarima$residuals, lag.max = 100, ci=0.99, main = "White Noise")  #Looks like white noise
autoplot(pacfmod)


Box.test(model_sarima$residuals)
